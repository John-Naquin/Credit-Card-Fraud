# -*- coding: utf-8 -*-
"""Machine Learning Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GZGt3PXCvO2vxEajZxmipCuWdAVje_XT

# Import libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix
from sklearn.impute import SimpleImputer
from sklearn import preprocessing
from sklearn import svm
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler, SMOTE
from collections import Counter
from google.colab import drive
from math import pi
import time

"""# Import Kaggle dataset"""

# Link to File: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud
df_train = pd.read_csv('/content/creditcard.csv')
df_train.shape

"""# Determine correlation coefficient of columns"""

# Calculate correlations
correlations = df_train.corr()['Class']
correlations = correlations.sort_values(ascending = False)

# Create heatmap of correlation values
correlation_full_health = df_train.corr()
axis_corr = sns.heatmap(
correlation_full_health,
vmin=-1, vmax=1, center=0,
cmap=sns.diverging_palette(50, 500, n=500),
square=True
)

plt.show()

# Separate the data into fraud and non-fraud
fraud_df = df_train[df_train['Class'] == 1]      # Fraud cases
non_fraud_df = df_train[df_train['Class'] == 0]  # Non-fraud cases

# Generate descriptive statistics for both
fraud_description = fraud_df['Amount'].describe()
non_fraud_description = non_fraud_df['Amount'].describe()

# Combine descriptions into a single table
summary = pd.concat([fraud_description, non_fraud_description], axis=1, keys=["Fraud", "Non-Fraud"])

# Display the side-by-side summary
print(summary)

"""# Compare the outlier percentages with the correlation coefficients and filter out columns whose correlation < .10"""

def outlier_percentage(column):
    # Calculate the IQR
    Q1 = column.quantile(0.25)
    Q3 = column.quantile(0.75)
    IQR = Q3 - Q1

    # Calculate the upper and lower bounds
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Identify outliers
    outliers = column[(column < lower_bound) | (column > upper_bound)]

    # Calculate percentage of outliers
    percentage = (len(outliers) / len(column)) * 100
    return percentage

outlier_percentages = df_train.apply(outlier_percentage)
outlier_percentages = outlier_percentages.sort_values(ascending=False)

comparison_df = pd.DataFrame({
    'Correlation': correlations,
    'Outlier Percentage': outlier_percentages
})

# Drop any rows with missing data (in case some features have NaNs)
comparison_df = comparison_df.dropna()
removed_comp = comparison_df.dropna()
comparison_df = comparison_df[comparison_df['Correlation'].abs() > 0.10]
comparison_df = comparison_df.sort_values(by=['Correlation', 'Outlier Percentage'], ascending=[False, False])

removed_comp = removed_comp[removed_comp['Correlation'].abs() < 0.10]
removed_comp = removed_comp.sort_values(by=['Correlation', 'Outlier Percentage'], ascending=[False, False])

# Get the list of columns that meet the criteria
columns_to_keep = comparison_df.index.tolist()

filtered_df = df_train[columns_to_keep]

print(comparison_df)

"""# Separate the filtered data into fraud and not fraud"""

# Split filtered_df into two DataFrames based on class labels
fraud_df = filtered_df[filtered_df['Class'] == 1]
notfraud_df = filtered_df[filtered_df['Class'] == 0]

#graph fraud data
# Initialize a 3x3 chart
fig, axes = plt.subplots(nrows=8, ncols=4, figsize=(8, 16))

# Flatten the axes array (makes it easier to iterate over)
axes = axes.flatten()

for i, column in enumerate(fraud_df.columns):
    # Add the histogram
    fraud_df[column].hist(ax=axes[i],
                    edgecolor='white',
                    color='#69b3a2'
                   )

    # Add title and axis label
    axes[i].set_title(f'{column}')
    axes[i].set_xlabel(column)
    axes[i].set_ylabel('Frequency')
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
fraud_df.describe()

#graph non_fraud data
fig, axes = plt.subplots(nrows=8, ncols=4, figsize=(8, 16))

# Flatten the axes array (makes it easier to iterate over)
axes = axes.flatten()

for i, column in enumerate(notfraud_df.columns):

    # Add the histogram
    notfraud_df[column].hist(ax=axes[i], # Define on which ax we're working on
                    edgecolor='white', # Color of the border
                    color='#69b3a2' # Color of the bins
                   )

    # Add title and axis label
    axes[i].set_title(f'{column}')
    axes[i].set_xlabel(column)
    axes[i].set_ylabel('Frequency')

for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
notfraud_df.describe()

"""# Use ROS and RUS to balance the data"""

f_train = filtered_df
f_train_X = f_train.drop(columns=['Class'])
f_train_y = f_train['Class'].dropna()



X = f_train.drop(columns=['Class'])
y = f_train['Class'].dropna()
X = X.loc[y.index]

rus = RandomUnderSampler(random_state=0)
X_under_resampled, y_under_resampled = rus.fit_resample(X, y)

ros = RandomOverSampler(random_state=0)
X_over_resampled, y_over_resampled = ros.fit_resample(X, y)

under_sampled_df = pd.concat([X_under_resampled, y_under_resampled], axis=1)
over_sampled_df = pd.concat([X_over_resampled, y_over_resampled], axis=1)

under_sampled_df.to_csv('under_sampled_creditcard.csv', index=False)
over_sampled_df.to_csv('over_sampled_creditcard.csv', index=False)

fraud_df = over_sampled_df[over_sampled_df['Class'] == 1]
notfraud_df = over_sampled_df[over_sampled_df['Class'] == 0]

fraud_df.describe()

#under_sampled_df.describe()
over_sampled_df.describe()
#filtered_df.describe()

# Example: Assuming `oversampled_df` and `undersampled_df` are your datasets
# Calculate the mean for each column
oversampled_means = over_sampled_df.mean(axis=0)
undersampled_means = under_sampled_df.mean(axis=0)

# Calculate distances from the diagonal line (y = x)
distances = np.abs(oversampled_means - undersampled_means) / np.sqrt(2)

# Create a scatter plot
plt.figure(figsize=(8, 6))
scatter = plt.scatter(oversampled_means, undersampled_means, c=distances, cmap='viridis', s=50)

# Add a colorbar to indicate distance magnitude
cbar = plt.colorbar(scatter)
cbar.set_label('Distance from Diagonal (Oversampled = Undersampled)')

# Annotate points with column names and distances
for col, x, y, dist in zip(oversampled_means.index, oversampled_means, undersampled_means, distances):
    plt.text(x, y, f"{col}", fontsize=8, ha='right', va='bottom')

# Add a diagonal reference line
plt.axline((0, 0), slope=1, color='red', linestyle='--', linewidth=1)

# Add labels and title
plt.xlabel('Mean (Oversampled Dataset)')
plt.ylabel('Mean (Undersampled Dataset)')
plt.title('Comparison of Column Means: Oversampled vs Undersampled')
plt.grid()
plt.show()

"""# Draw boxplots of over and undersampled data"""

box_under = under_sampled_df.loc[:, under_sampled_df.columns]
box_under = box_under.loc[:, box_under.columns != 'Class']

# Number of columns per row
cols_per_row = 6

# Calculate the number of rows needed
num_rows = (len(box_under.columns) + cols_per_row - 1) // cols_per_row

# Create subplots
fig, axes = plt.subplots(num_rows, cols_per_row, figsize=(20, num_rows * 5))

# Flatten the axes array for easy iteration
axes = axes.flatten()

for i, c in enumerate(box_under.columns):
    box_under.boxplot(column=c, ax=axes[i])
    axes[i].set_title(c)

# Hide any unused subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])
plt.tight_layout()
plt.show()

box_over = over_sampled_df.loc[:, over_sampled_df.columns]
box_over = box_over.loc[:, box_over.columns != 'Class']

# Number of columns per row
cols_per_row = 6

# Calculate the number of rows needed
num_rows = (len(box_under.columns) + cols_per_row - 1) // cols_per_row

# Create subplots
fig, axes = plt.subplots(num_rows, cols_per_row, figsize=(20, num_rows * 5))

# Flatten the axes array for easy iteration
axes = axes.flatten()

for i, c in enumerate(box_under.columns):
    box_over.boxplot(column=c, ax=axes[i])
    axes[i].set_title(c)

# Hide any unused subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])
plt.tight_layout()
plt.show()

"""# Comparing Models Performance on Unbalanced Data"""

confusion_matrices = []
model_names = []

models = {
    'Logistic Regression': LogisticRegression(max_iter=5000, random_state=0),
    'Decision Tree': DecisionTreeClassifier(random_state=0),
    'Random Forest': RandomForestClassifier(random_state=0),
    'Neural Network': MLPClassifier(max_iter=1000, random_state=0),
    'SVM: Linear Kernel': SVC(kernel='linear', random_state=0),
    'SVM: Poly Kernel': SVC(kernel='poly', random_state=0),
    'SVM: RBF Kernel': SVC(kernel='rbf', random_state=0)
}

def evaluate_models(models, X, y, sampling_type):
    results = []
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

    for model_name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        accuracy = accuracy_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred, pos_label=1)
        precision = precision_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        cm = confusion_matrix(y_test, y_pred)
        tn, fp, fn, tp = cm.ravel()

        confusion_matrices.append(cm)
        model_names.append(model_name)

        fraud_as_legit = fn / (fn + tp) if (fn + tp) > 0 else 0
        legit_as_fraud = fp / (fp + tn) if (fp + tn) > 0 else 0

        results.append({
            'Model': model_name,
            'Accuracy': accuracy,
            'Recall': recall,
            'Precision': precision,
            'Fraud as Legit (%)': fraud_as_legit * 100,
            'Legit as Fraud (%)': legit_as_fraud * 100,
            'F1 Score': f1
        })

    df = pd.DataFrame(results)
    svm_row = df[df['Model'] == 'SVM: Linear Kernel']

    if not svm_row.empty:
        svm_accuracy = svm_row['Accuracy'].values[0]
        svm_recall = svm_row['Recall'].values[0]
        svm_precision = svm_row['Precision'].values[0]
        svm_fraud_as_legit = svm_row['Fraud as Legit (%)'].values[0]
        svm_legit_as_fraud = svm_row['Legit as Fraud (%)'].values[0]
        svm_f1_score = svm_row['F1 Score'].values[0]

        #df['Accuracy vs SVM'] = df['Accuracy'] - svm_accuracy
        #df['Fraud as Legit vs SVM (%)'] = df['Fraud as Legit (%)'] - svm_fraud_as_legit
        #df['Legit as Fraud vs SVM (%)'] = df['Legit as Fraud (%)'] - svm_legit_as_fraud

    n = len(confusion_matrices)
    cols = 3
    rows = (n + cols - 1) // cols

    fig, axes = plt.subplots(rows, cols, figsize=(10, 5 * rows))
    axes = axes.flatten()

    for i, cm in enumerate(confusion_matrices):
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[i])
        axes[i].set_title(f'{model_names[i]} ({sampling_type})')
        axes[i].set_xlabel('Predicted')
        axes[i].set_ylabel('Actual')

    for j in range(len(confusion_matrices), len(axes)):
        axes[j].axis('off')

    plt.tight_layout()
    # plt.savefig(f'{sampling_type}_confusion_matrices.png') # # Saves a png of the heatmaps
    plt.show()

    return df

def plot_heatmap(results, title):
    plt.figure(figsize=(12, 8))
    sns.heatmap(results.set_index('Model').T, annot=True, cmap='Blues', linewidths=0.5, fmt='.4f')
    plt.title(f'{title} - Metrics Heatmap', size=20)
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    # plt.savefig(f'{title.replace(" ", "_")}.png') # Saves a png of the heatmaps
    plt.show()

all_results = evaluate_models(models,f_train_X, f_train_y, 'All Data')
plot_heatmap(all_results, "All Data Model Evaluation Results")
#print("\All Data Model Evaluation Results Compared to SVM:\n", all_results)

"""Comparing Models Performance On Over and Under Sampling"""

X_under = under_sampled_df.drop(columns=['Class'])
y_under = under_sampled_df['Class']

scaler = StandardScaler()
X_under = scaler.fit_transform(X_under)

models = {
    'Logistic Regression': LogisticRegression(max_iter=5000, random_state=0),
    'Decision Tree': DecisionTreeClassifier(random_state=0),
    'Random Forest': RandomForestClassifier(random_state=0),
    'Neural Network': MLPClassifier(max_iter=1000, random_state=0),
    'SVM: Linear Kernel': SVC(kernel='linear', random_state=0),
    'SVM: Poly Kernel': SVC(kernel='poly', random_state=0),
    'SVM: RBF Kernel': SVC(kernel='rbf', random_state=0)
}




confusion_matrices = []
model_names = []

def evaluate_models(models, X, y, sampling_type):
    results = []
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

    for model_name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        accuracy = accuracy_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred, pos_label=1)
        precision = precision_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        cm = confusion_matrix(y_test, y_pred)
        tn, fp, fn, tp = cm.ravel()

        confusion_matrices.append(cm)
        model_names.append(model_name)

        fraud_as_legit = fn / (fn + tp) if (fn + tp) > 0 else 0
        legit_as_fraud = fp / (fp + tn) if (fp + tn) > 0 else 0

        results.append({
            'Model': model_name,
            'Accuracy': accuracy,
            'Recall': recall,
            'Precision': precision,
            'Fraud as Legit (%)': fraud_as_legit * 100,
            'Legit as Fraud (%)': legit_as_fraud * 100,
            'F1 Score': f1
        })

    df = pd.DataFrame(results)
    svm_row = df[df['Model'] == 'SVM: Linear Kernel']

    if not svm_row.empty:
        svm_accuracy = svm_row['Accuracy'].values[0]
        svm_recall = svm_row['Recall'].values[0]
        svm_precision = svm_row['Precision'].values[0]
        svm_fraud_as_legit = svm_row['Fraud as Legit (%)'].values[0]
        svm_legit_as_fraud = svm_row['Legit as Fraud (%)'].values[0]
        svm_f1_score = svm_row['F1 Score'].values[0]

        #df['Accuracy vs SVM'] = df['Accuracy'] - svm_accuracy
        #df['Fraud as Legit vs SVM (%)'] = df['Fraud as Legit (%)'] - svm_fraud_as_legit
        #df['Legit as Fraud vs SVM (%)'] = df['Legit as Fraud (%)'] - svm_legit_as_fraud

    n = len(confusion_matrices)
    cols = 3
    rows = (n + cols - 1) // cols

    fig, axes = plt.subplots(rows, cols, figsize=(10, 5 * rows))
    axes = axes.flatten()

    for i, cm in enumerate(confusion_matrices):
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[i])
        axes[i].set_title(f'{model_names[i]} ({sampling_type})')
        axes[i].set_xlabel('Predicted')
        axes[i].set_ylabel('Actual')

    # Turn off unused subplots (if any)
    for j in range(len(confusion_matrices), len(axes)):
        axes[j].axis('off')

    plt.tight_layout()
    # plt.savefig(f'{sampling_type}_confusion_matrices.png') # # Saves a png of the heatmaps
    plt.show()

    return df

def plot_heatmap(results, title):
    plt.figure(figsize=(12, 8))
    sns.heatmap(results.set_index('Model').T, annot=True, cmap='Blues', linewidths=0.5, fmt='.4f')
    plt.title(f'{title} - Metrics Heatmap', size=20)
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    # plt.savefig(f'{title.replace(" ", "_")}.png') # Saves a png of the heatmaps
    plt.show()

under_results = evaluate_models(models, X_under, y_under, 'Under-Sampled')
plot_heatmap(under_results, "Under-Sampled Model Evaluation Results")

#print("\nUnder-Sampled Model Evaluation Results Compared to SVM:\n", under_results)
#under_results.to_csv('under_sampled_results.csv', index=False)  # This downloads a cvs of the under sampled results

"""# Comparing Models Performance on Under and Over Sampling"""

X_under = under_sampled_df.drop(columns=['Class'])
y_under = under_sampled_df['Class']

X_over = over_sampled_df.drop(columns=['Class'])
y_over = over_sampled_df['Class']

scaler = StandardScaler()
X_under_scaled = scaler.fit_transform(X_under)

pca = PCA(n_components=10)
X_over_reduced = pca.fit_transform(X_over)

smote = SMOTE(k_neighbors=3, random_state=0)
X_over_resampled, y_over_resampled = smote.fit_resample(X_over_reduced, y_over)

X_over_scaled = scaler.fit_transform(X_over_resampled)

models = {
    'Logistic Regression': LogisticRegression(max_iter=5000, random_state=0),
    'Neural Network': MLPClassifier(max_iter=1000, random_state=0),
    'Decision Tree': DecisionTreeClassifier(random_state=0),
    'Random Forest': RandomForestClassifier(random_state=0),
    'SVM: Linear Kernel': SVC(kernel='linear', random_state=0),
    'SVM: Poly Kernel': SVC(kernel='poly', random_state=0),
    'SVM: RBF Kernel': SVC(kernel='rbf', random_state=0)
}

def evaluate_models(models, X, y, sampling_type):
    print(f"\n--- Starting {sampling_type} Model Evaluation ---")
    start_total_time = time.time()
    results = []
    confusion_matrices = []
    model_names = []

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

    total_models = len(models)
    for i, (model_name, model) in enumerate(models.items(), 1):
        print(f"Processing model {i}/{total_models}: {model_name}")
        model_start_time = time.time()

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        accuracy = accuracy_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred, pos_label=1)
        precision = precision_score(y_test, y_pred, pos_label=1)
        f1 = f1_score(y_test, y_pred, pos_label=1)
        cm = confusion_matrix(y_test, y_pred)
        tn, fp, fn, tp = cm.ravel()

        confusion_matrices.append(cm)
        model_names.append(model_name)

        fraud_as_legit = fn / (fn + tp) if (fn + tp) > 0 else 0
        legit_as_fraud = fp / (fp + tn) if (fp + tn) > 0 else 0

        results.append({
            'Model': model_name,
            'Accuracy': accuracy,
            'Recall': recall,
            'Precision': precision,
            'Fraud as Legit (%)': fraud_as_legit * 100,
            'Legit as Fraud (%)': legit_as_fraud * 100,
            'F1 Score': f1
        })

        model_end_time = time.time()
        print(f"Completed {model_name} in {model_end_time - model_start_time:.2f} seconds")

    df = pd.DataFrame(results)
    svm_row = df[df['Model'] == 'SVM: Linear Kernel']

    if not svm_row.empty:
        svm_accuracy = svm_row['Accuracy'].values[0]
        df['Accuracy vs SVM'] = df['Accuracy'] - svm_accuracy
        df['Fraud as Legit vs SVM (%)'] = df['Fraud as Legit (%)'] - svm_row['Fraud as Legit (%)'].values[0]
        df['Legit as Fraud vs SVM (%)'] = df['Legit as Fraud (%)'] - svm_row['Legit as Fraud (%)'].values[0]

    n = len(confusion_matrices)
    cols = 3
    rows = (n + cols - 1) // cols

    fig, axes = plt.subplots(rows, cols, figsize=(10, 5 * rows))
    axes = axes.flatten()

    for i, cm in enumerate(confusion_matrices):
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[i])
        axes[i].set_title(f'{model_names[i]} ({sampling_type})')
        axes[i].set_xlabel('Predicted')
        axes[i].set_ylabel('Actual')

    for j in range(len(confusion_matrices), len(axes)):
        axes[j].axis('off')

    plt.tight_layout()
    plt.show()

    end_total_time = time.time()
    print(f"\n--- Completed {sampling_type} Model Evaluation in {end_total_time - start_total_time:.2f} seconds ---")

    return df

def plot_heatmap(results, title):
    plt.figure(figsize=(12, 8))
    sns.heatmap(results.set_index('Model').T, annot=True, cmap='Blues', linewidths=0.5, fmt='.4f')
    plt.title(f'{title} - Metrics Heatmap', size=20)
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

under_results = evaluate_models(models, X_under_scaled, y_under, 'Under-Sampled')
plot_heatmap(under_results, "Under-Sampled Model Evaluation Results")

over_results = evaluate_models(models, X_over_scaled, y_over_resampled, 'Over-Sampled')
plot_heatmap(over_results, "Over-Sampled Model Evaluation Results")

print("\nUnder-Sampled Model Evaluation Results Compared to SVM:\n", under_results)
print("\nOver-Sampled Model Evaluation Results Compared to SVM:\n", over_results)

"""# Comparing Models Performance on Under and Over Sampling (WITH TWEAKS)"""

X_under = under_sampled_df.drop(columns=['Class'])
y_under = under_sampled_df['Class']

X_over = over_sampled_df.drop(columns=['Class'])
y_over = over_sampled_df['Class']

scaler = StandardScaler()
X_under_scaled = scaler.fit_transform(X_under)

pca = PCA(n_components=10)
X_over_reduced = pca.fit_transform(X_over)

smote = SMOTE(k_neighbors=3, random_state=0)
X_over_resampled, y_over_resampled = smote.fit_resample(X_over_reduced, y_over)

X_over_scaled = scaler.fit_transform(X_over_resampled)

models = {
    'Logistic Regression': LogisticRegression(max_iter=5000, random_state=0),
    'Neural Network': MLPClassifier(max_iter=1000, random_state=0),
    'Decision Tree': DecisionTreeClassifier(random_state=0),
    'Random Forest': RandomForestClassifier(random_state=0),
    'SVM: Linear Kernel': SVC(kernel='linear', C=1.0, random_state=0),
    'SVM: Poly Kernel': SVC(kernel='poly', degree=3, C=1.0, gamma='scale', random_state=0),
    'SVM: RBF Kernel': SVC(kernel='rbf', C=1.0, gamma='scale', random_state=0)
}

def evaluate_models(models, X, y, sampling_type):
    print(f"\n--- Starting {sampling_type} Model Evaluation ---")
    start_total_time = time.time()
    results = []
    confusion_matrices = []
    model_names = []

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

    total_models = len(models)
    for i, (model_name, model) in enumerate(models.items(), 1):
        print(f"Processing model {i}/{total_models}: {model_name}")
        model_start_time = time.time()

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        accuracy = accuracy_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred, pos_label=1)
        precision = precision_score(y_test, y_pred, pos_label=1)
        f1 = f1_score(y_test, y_pred, pos_label=1)
        cm = confusion_matrix(y_test, y_pred)
        tn, fp, fn, tp = cm.ravel()

        confusion_matrices.append(cm)
        model_names.append(model_name)

        fraud_as_legit = fn / (fn + tp) if (fn + tp) > 0 else 0
        legit_as_fraud = fp / (fp + tn) if (fp + tn) > 0 else 0

        results.append({
            'Model': model_name,
            'Accuracy': accuracy,
            'Recall': recall,
            'Precision': precision,
            'Fraud as Legit (%)': fraud_as_legit * 100,
            'Legit as Fraud (%)': legit_as_fraud * 100,
            'F1 Score': f1
        })

        model_end_time = time.time()
        print(f"Completed {model_name} in {model_end_time - model_start_time:.2f} seconds")

    df = pd.DataFrame(results)
    svm_row = df[df['Model'] == 'SVM: Linear Kernel']

    if not svm_row.empty:
        svm_accuracy = svm_row['Accuracy'].values[0]
        df['Accuracy vs SVM'] = df['Accuracy'] - svm_accuracy
        df['Fraud as Legit vs SVM (%)'] = df['Fraud as Legit (%)'] - svm_row['Fraud as Legit (%)'].values[0]
        df['Legit as Fraud vs SVM (%)'] = df['Legit as Fraud (%)'] - svm_row['Legit as Fraud (%)'].values[0]

    n = len(confusion_matrices)
    cols = 3
    rows = (n + cols - 1) // cols

    fig, axes = plt.subplots(rows, cols, figsize=(10, 5 * rows))
    axes = axes.flatten()

    for i, cm in enumerate(confusion_matrices):
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[i])
        axes[i].set_title(f'{model_names[i]} ({sampling_type})')
        axes[i].set_xlabel('Predicted')
        axes[i].set_ylabel('Actual')

    for j in range(len(confusion_matrices), len(axes)):
        axes[j].axis('off')

    plt.tight_layout()
    plt.show()

    end_total_time = time.time()
    print(f"\n--- Completed {sampling_type} Model Evaluation in {end_total_time - start_total_time:.2f} seconds ---")

    return df

def plot_heatmap(results, title):
    plt.figure(figsize=(12, 8))
    sns.heatmap(results.set_index('Model').T, annot=True, cmap='Blues', linewidths=0.5, fmt='.4f')
    plt.title(f'{title} - Metrics Heatmap', size=20)
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

under_results = evaluate_models(models, X_under_scaled, y_under, 'Under-Sampled')
plot_heatmap(under_results, "Under-Sampled Model Evaluation Results")

over_results = evaluate_models(models, X_over_scaled, y_over_resampled, 'Over-Sampled')
plot_heatmap(over_results, "Over-Sampled Model Evaluation Results")

print("\nUnder-Sampled Model Evaluation Results Compared to SVM:\n", under_results)
print("\nOver-Sampled Model Evaluation Results Compared to SVM:\n", over_results)

